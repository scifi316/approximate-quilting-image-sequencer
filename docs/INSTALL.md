# Installation
This is for installing the python 3.12 version of the program, please make sure all the requirements are met.

### Basic Requirements 
- NVIDIA GPU with CUDA 10.X.X capabilities
- Python 3.12.X+ 
- Anaconda 4.X+
- At least 25GB of storage available
- FFMPEG full release

### Setup
After making sure all of those requirements are alerady met you can start by cloning this repo

$ ```git clone https://github.com/scifi316/approximate-quilting-image-sequencer.git```

Download the correct version of necessary binaries from the [Releases](https://github.com/scifi316/approximate-quilting-image-sequencer/releases/tag/database) tab of the repo as well. 
- ___Current release data package is V1___

You will need to extract the binaries and install them in the correct locations.

You should be left with the files:
- frame_ids.npy
- frame_to_descriptor_indicies.npy
- individual_descriptor_faiss_index.bin
- input.zip

Create a folder in the root directory called `data`, then create a folder inside that called `images`

Extract the input.zip file and copy the contents into the `images` folder, while in the images directory. Create a folder called `quilted_output` and `source`, the current file structure should look like this:

- /root_folder/
    - src
      - database
      - ...
    - data
      - images
        - input
          - frame0000.png
          - frame0001.png
          - ...
        - quilted_output
        - source
    - tests
    - etc...

After creating and transferring the necessary files, navigate back to the root directory of the project where all the READMEs and LICENSE stuff is and copy the `frame_ids.npy`,
`frame_to_descriptor_indicies.npy`, and
`individual_descriptor_faiss_index.bin` to the folder.

_Now we should be able to start running some commands..._

In the root directory, open a terminal instance and run:

$ ```conda env create -f requirements.yml```

*Take a break as this may take a while to install the necessary python libraries, after it has finished installing, run: 

$ ```conda activate```

We should now have a proper working environemnt to run, test, and debug in. 

### Modifying and testing
__NOTE:__ Make sure you use the "activated" terminal to run the python files as it may fail to load the necessary libraries during certain steps.

Any changes made to the input database located in `/data/images/input` requires a rebuild of the database binaries and headings. To accomplish such, navigate to the folder `/src/test/proto` and run the `poc1.py` file to rebuild the database, this will take some time but should generate the new binaries/headings.

To generate a video, make sure [FFMPEG](https://www.ffmpeg.org/download.html) is already installed on your system. This program works by sequentially generating each frame from a source video using the database of images given. 

This requires take your video source and splitting them into individual frames for processing. The simplest way is by opening a terminal in the `.../source` directory you should have already created. There copy your video to that directory, `"video_name.mp4"` and run the following FFMPEG command:

$ ```ffmpeg -i "video_name.mp4" -vf fps=[FPS of video] frame%04d.png```

Where `"video_name.mp4"` is the name of the video you want to use in MP4 format, and the `[FPS of video]` is the video's framerate in whole number format (24, 30, 60, etc.), this FPS is important for correctly generating the frames needed with FFMPEG. 

Depending on the specs of your PC and the video you use, this may take a moment for FFMPEG to parse and split the video into the correct amount of frames. 

__NOTE:__ Currently this program only supports frame generation with video resolutions of 1920x1080, this program is also very slow and only does 2-3 FPS, so longer videos and/or videos with higher framerates will take longer time.

_Now_ we should be able to create some new frames, locate the file `stitch.py` in the directory `/src/tests/proto/`, if everything has been done correctly, running `stitch.py` through the CLI/Terminal should start generating new frames in the folder `quilted_output`. It will prompt you with the message `"Quilted all images"` when finished.

To rebuild the frames back into a video, navigate to the `"quilted_output"` directory and open a terminal instance, there we run:

$ ```ffmpeg -framerate [FPS of video] -i quilted_frame%04d.png -c:v libx264 -pix_fmt yuv420p out.mp4```

Where `"FPS of video"` is the original framerate of the video you wanted to generate during the splitting process earlier; after a moment FFMPEG will have generated a file called `"out.mp4"`, this is your video, open it to make sure it was correctly generated by FFMPEG.

